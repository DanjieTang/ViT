{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hmf43eOzQQUU"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "epochs = 10\n",
    "batch_size = 150\n",
    "lr = 3e-4\n",
    "weight_decay = 0.01\n",
    "device = \"mps\"\n",
    "checkpoint_filepath = \"\"  # Set to a path if you want to load a checkpoint\n",
    "save_dir = \"checkpoints\"\n",
    "dataset_filepath = \"./ImagenetHighResolution\"\n",
    "import os\n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfdeGJwXQIHU"
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that converts image patches to embeddings for Vision Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 image_size: tuple = (64, 72),\n",
    "                 patch_size: int = 16,\n",
    "                 in_channels: int = 3,\n",
    "                 embedding_dim: int = 1024):\n",
    "        super().__init__()\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.in_channels = in_channels\n",
    "\n",
    "        # Calculate number of patches\n",
    "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size)\n",
    "\n",
    "        # Create projection for converting patches to embeddings\n",
    "        self.projection = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embedding_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "\n",
    "        # CLS token embedding\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "\n",
    "        # Positional embedding (Normal distribution initialization of value)\n",
    "        self.positions = nn.Parameter(torch.zeros(1, self.num_patches + 1, embedding_dim))\n",
    "        nn.init.trunc_normal_(self.positions, std=0.02)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        # Convert image to patches and project to embedding dimension\n",
    "        # x shape: [batch_size, channels, height, width]\n",
    "        x = self.projection(x)\n",
    "        # x shape: [batch_size, embedding_dim, height/patch_size, width/patch_size]\n",
    "\n",
    "        # Flatten patches to sequence\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        # x shape: [batch_size, num_patches, embedding_dim]\n",
    "\n",
    "        # Add CLS token\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # Add positional embeddings\n",
    "        x = x + self.positions\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.head_dim = head_dim\n",
    "        self.q_head = q_head\n",
    "        self.kv_head = kv_head\n",
    "        self.qkv = nn.Linear(hidden_dim, (q_head+kv_head*2)*head_dim)\n",
    "        self.o = nn.Linear(q_head*head_dim, hidden_dim)\n",
    "        self.scaler = 1/math.sqrt(head_dim)\n",
    "        self.lora_qkv_a = nn.Linear(hidden_dim, lora_rank)\n",
    "        self.lora_qkv_b = nn.Linear(lora_rank, (q_head+kv_head*2)*head_dim)\n",
    "        self.lora_o_a = nn.Linear(q_head*head_dim, lora_rank)\n",
    "        self.lora_o_b = nn.Linear(lora_rank, hidden_dim)\n",
    "\n",
    "        if q_head != kv_head:\n",
    "            # If we are using multi query attention\n",
    "            assert q_head % kv_head == 0\n",
    "            self.multi_query_attention = True\n",
    "            self.q_kv_scale = q_head//kv_head\n",
    "        else:\n",
    "            self.multi_query_attention = False\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        batch_size, seq_len, hid_dim = tensor.shape\n",
    "\n",
    "        qkv_tensor = self.qkv(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_qkv_a(tensor)\n",
    "            lora_tensor = self.lora_qkv_b(lora_tensor)\n",
    "            qkv_tensor = lora_tensor + qkv_tensor\n",
    "        query, key, value = qkv_tensor.split([self.head_dim*self.q_head, self.head_dim*self.kv_head, self.head_dim*self.kv_head], dim=-1)\n",
    "\n",
    "        query = query.view(batch_size, seq_len, self.q_head, self.head_dim)\n",
    "        key = key.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "        value = value.view(batch_size, seq_len, self.kv_head, self.head_dim)\n",
    "\n",
    "        if self.multi_query_attention:\n",
    "            # If we are using multi query attention, duplicate key value heads\n",
    "            key = torch.repeat_interleave(key, self.q_kv_scale, dim=-2)\n",
    "            value = torch.repeat_interleave(value, self.q_kv_scale, dim=-2)\n",
    "\n",
    "        # Switch to batch_size, head, seq_len, head_dim\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Classic self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scaler\n",
    "        if attention_mask != None:\n",
    "            attention_scaled += attention_mask\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape back to batch_size, seq_len, hid_dim\n",
    "        value = value.transpose(1, 2).contiguous()\n",
    "        value = value.view(batch_size, seq_len, hid_dim)\n",
    "\n",
    "        # Output layer\n",
    "        output = self.o(value)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_o_a(value)\n",
    "            lora_tensor = self.lora_o_b(lora_tensor)\n",
    "            output = lora_tensor + output\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_size: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.gate_and_up = nn.Linear(hidden_size, hidden_size * expansion_factor * 2)\n",
    "        self.down = nn.Linear(hidden_size * expansion_factor, hidden_size)\n",
    "        self.dropout = nn.Dropout(p=dropout_ratio)\n",
    "        self.lora_gate_and_up_a = nn.Linear(hidden_size, lora_rank)\n",
    "        self.lora_gate_and_up_b = nn.Linear(lora_rank, hidden_size * expansion_factor * 2)\n",
    "        self.lora_down_a = nn.Linear(hidden_size * expansion_factor, lora_rank)\n",
    "        self.lora_down_b = nn.Linear(lora_rank, hidden_size)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> torch.Tensor:\n",
    "        gate_and_up = self.gate_and_up(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_gate_and_up_a(tensor)\n",
    "            lora_tensor = self.lora_gate_and_up_b(lora_tensor)\n",
    "            gate_and_up = gate_and_up + lora_tensor\n",
    "        gate, up = gate_and_up.chunk(chunks=2, dim=-1)\n",
    "        gate = F.gelu(gate, approximate=\"tanh\")\n",
    "        tensor = gate * up\n",
    "        tensor = self.dropout(tensor)\n",
    "        down_tensor = self.down(tensor)\n",
    "        if fine_tuning:\n",
    "            lora_tensor = self.lora_down_a(tensor)\n",
    "            lora_tensor = self.lora_down_b(lora_tensor)\n",
    "            down_tensor = down_tensor + lora_tensor\n",
    "        return down_tensor\n",
    "\n",
    "\n",
    "class MOE(nn.Module):\n",
    "    def __init__(self, hidden_size: int, num_experts: int = 8, expansion_factor: int = 4, dropout_ratio: float = 0.1, lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.gate = nn.Linear(hidden_size, num_experts)\n",
    "        self.num_experts = num_experts\n",
    "        self.experts = nn.ModuleList([FeedForward(hidden_size, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio, lora_rank=lora_rank) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, fine_tuning: bool = False) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Flatten for better manipulation, this is ok because tokens are independent at this stage\n",
    "        batch_size, seq_len, hidden_size = tensor.shape\n",
    "        flat_tensor = tensor.reshape(batch_size * seq_len, hidden_size)\n",
    "\n",
    "        # Pass through the gating network and select experts\n",
    "        tensor = self.gate(flat_tensor)\n",
    "        tensor = F.softmax(tensor, dim=-1)\n",
    "\n",
    "        # The output of this step is a tensor of shape [batch_size * seq_len, 2] with element i in the second dimension representing ith expert selected for this token\n",
    "        value_tensor, index_tensor = tensor.topk(k=2, dim=-1)\n",
    "\n",
    "        # Find the load balancing loss\n",
    "        counts = torch.bincount(index_tensor[:, 0], minlength=self.num_experts)\n",
    "        frequencies = counts.float() / (batch_size * seq_len) # This is the hard one-hot frequency\n",
    "        probability = tensor.mean(0) # This is the soft probability\n",
    "        load_balancing_loss = (probability * frequencies).mean() * float(self.num_experts ** 2)\n",
    "\n",
    "        # Normalize top1 and top2 score\n",
    "        top_expert_score = value_tensor[:, 0]\n",
    "        second_expert_score = value_tensor[:, 1]\n",
    "        total_score = top_expert_score + second_expert_score\n",
    "        top_expert_score = top_expert_score / total_score\n",
    "        second_expert_score = second_expert_score / total_score\n",
    "\n",
    "        # Split into top 2 experts\n",
    "        split_tensors = torch.split(index_tensor, 1, dim=-1)\n",
    "        top_expert, second_expert = split_tensors[0], split_tensors[1]\n",
    "        indices = torch.arange(batch_size * seq_len).unsqueeze(-1).to(tensor.device)\n",
    "        top_expert = torch.cat((indices, top_expert), dim=-1)\n",
    "        second_expert = torch.cat((indices, second_expert), dim=-1)\n",
    "\n",
    "        # Sort based on expert selection\n",
    "        top_expert = top_expert[top_expert[:,1].argsort()]\n",
    "        second_expert = second_expert[second_expert[:,1].argsort()]\n",
    "\n",
    "        # Count how many tokens goes to each expert\n",
    "        top_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            top_expert_counts[i] = (top_expert[:,1] == i).sum()\n",
    "        top_expert_counts = top_expert_counts.tolist()\n",
    "\n",
    "        second_expert_counts = torch.zeros(self.num_experts, dtype=int)\n",
    "        for i in range(self.num_experts):\n",
    "            second_expert_counts[i] = (second_expert[:,1] == i).sum()\n",
    "        second_expert_counts = second_expert_counts.tolist()\n",
    "\n",
    "        # Split input tokens for each expert\n",
    "        top_expert_tokens = flat_tensor[top_expert[:,0]]\n",
    "        second_expert_tokens = flat_tensor[second_expert[:,0]]\n",
    "\n",
    "        # Split into a list of tensors, element i tensor is for ith expert.\n",
    "        top_expert_tokens = torch.split(top_expert_tokens, top_expert_counts, dim=0)\n",
    "        second_expert_tokens = torch.split(second_expert_tokens, second_expert_counts, dim=0)\n",
    "\n",
    "        # Input into each expert and obtain results in a list\n",
    "        top_expert_outputs = [self.experts[i](top_expert_tokens[i], fine_tuning) if top_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(tensor.device) for i in range(self.num_experts)]\n",
    "        second_expert_outputs = [self.experts[i](second_expert_tokens[i], fine_tuning) if second_expert_counts[i] > 0 else torch.zeros(0, hidden_size, dtype=torch.float16).to(tensor.device) for i in range(self.num_experts)]\n",
    "\n",
    "        # Combine outputs\n",
    "        top_expert_outputs = torch.cat(top_expert_outputs, dim=0)\n",
    "        second_expert_outputs = torch.cat(second_expert_outputs, dim=0)\n",
    "\n",
    "        # Re-index the output back to original token order\n",
    "        # flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(tensor.device)\n",
    "        # flat_top_expert_tensor.index_copy_(0, top_expert[:, 0], top_expert_outputs)\n",
    "\n",
    "        # flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(tensor.device)\n",
    "        # flat_second_expert_tensor.index_copy_(0, second_expert[:, 0], second_expert_outputs)\n",
    "        flat_top_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(\"cpu\")\n",
    "        flat_top_expert_tensor = flat_top_expert_tensor.index_copy_(0, top_expert[:, 0].to(\"cpu\"), top_expert_outputs.to(\"cpu\")).to(tensor.device)\n",
    "        flat_second_expert_tensor = torch.zeros_like(flat_tensor, dtype=torch.float32).to(\"cpu\")\n",
    "        flat_second_expert_tensor = flat_second_expert_tensor.index_copy_(0, second_expert[:, 0].to(\"cpu\"), second_expert_outputs.to(\"cpu\")).to(tensor.device)\n",
    "\n",
    "        # Find final output tensor based on weight between top and second expert\n",
    "        final_tensor = top_expert_score.unsqueeze(-1) * flat_top_expert_tensor + second_expert_score.unsqueeze(-1) * flat_second_expert_tensor\n",
    "\n",
    "        # Reshape to original [batch_size, seq_len, hidden_size]\n",
    "        final_tensor = final_tensor.reshape(batch_size, seq_len, hidden_size)\n",
    "\n",
    "        return final_tensor, load_balancing_loss\n",
    "\n",
    "\n",
    "class VisionLayer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden_dim: int,\n",
    "                 head_dim: int,\n",
    "                 q_head: int,\n",
    "                 kv_head: int,\n",
    "                 expansion_factor: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = False,\n",
    "                 num_experts: int = 8,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.use_moe = use_moe\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.attention = VisionAttention(hidden_dim, head_dim, q_head, kv_head, lora_rank=lora_rank)\n",
    "\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        if self.use_moe:\n",
    "            self.moe = MOE(hidden_dim, num_experts=num_experts, expansion_factor=expansion_factor,\n",
    "                           dropout_ratio=dropout_ratio, lora_rank=lora_rank)\n",
    "        else:\n",
    "            self.ffn = FeedForward(hidden_dim, expansion_factor=expansion_factor, dropout_ratio=dropout_ratio,\n",
    "                                  lora_rank=lora_rank)\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor, attention_mask: torch.Tensor = None, fine_tuning: bool = False):\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = self.attention(tensor, attention_mask=attention_mask, fine_tuning=fine_tuning)\n",
    "        tensor += skip_connection\n",
    "\n",
    "        skip_connection = tensor\n",
    "        tensor = self.norm2(tensor)\n",
    "        if self.use_moe:\n",
    "            tensor, load_balancing_loss = self.moe(tensor, fine_tuning=fine_tuning)\n",
    "        else:\n",
    "            tensor = self.ffn(tensor, fine_tuning=fine_tuning)\n",
    "            load_balancing_loss = torch.tensor(0.0, dtype=tensor.dtype, device=tensor.device)\n",
    "\n",
    "        tensor += skip_connection\n",
    "\n",
    "        return tensor, load_balancing_loss\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 image_size: tuple,\n",
    "                 num_classes: int = 1,\n",
    "                 patch_size: int = 8,\n",
    "                 in_channels: int = 3,\n",
    "                 num_layer: int = 3,\n",
    "                 hidden_dim: int = 1024,\n",
    "                 expansion_factor: int = 8,\n",
    "                 head_dim: int = 64,\n",
    "                 q_head: int = 16,\n",
    "                 kv_head: int = 4,\n",
    "                 dropout_ratio: float = 0.1,\n",
    "                 use_moe: bool = True,\n",
    "                 num_experts: int = 8,\n",
    "                 load_balancing_loss_weight: float = 1e-2,\n",
    "                 fine_tuning: bool = False,\n",
    "                 lora_rank: int = 16):\n",
    "        super().__init__()\n",
    "        self.num_layer = num_layer\n",
    "        self.load_balancing_loss_weight = load_balancing_loss_weight\n",
    "        self.fine_tuning = fine_tuning\n",
    "\n",
    "        # Patch embedding\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            in_channels=in_channels,\n",
    "            embedding_dim=hidden_dim\n",
    "        )\n",
    "\n",
    "        # Calculate number of patches (sequence length)\n",
    "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size) + 1  # +1 for cls token\n",
    "\n",
    "        if q_head == None:\n",
    "            q_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if kv_head == None:\n",
    "            kv_head = (hidden_dim // head_dim)\n",
    "\n",
    "        if hidden_dim % (head_dim * q_head) != 0 or hidden_dim % (head_dim * kv_head):\n",
    "            raise ValueError(\"Error: hidden_dim or projection_dim (if specified) must be divisible by the product of the number of q or kv heads and the head dimension.\")\n",
    "\n",
    "        # Create transformer layers\n",
    "        self.transformer = nn.ModuleList()\n",
    "        for _ in range(self.num_layer):\n",
    "            self.transformer.append(VisionLayer(\n",
    "                hidden_dim, head_dim, q_head, kv_head,\n",
    "                expansion_factor=expansion_factor,\n",
    "                dropout_ratio=dropout_ratio,\n",
    "                use_moe=use_moe,\n",
    "                num_experts=num_experts,\n",
    "                lora_rank=lora_rank\n",
    "            ))\n",
    "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Final classifier head\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def begin_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = True\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"lora\" not in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def exit_fine_tunning(self) -> None:\n",
    "        self.fine_tuning = False\n",
    "        for name, param in self.named_parameters():\n",
    "            if \"positions\" in name:\n",
    "                param.requires_grad = False\n",
    "            else:\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # Handle input shape\n",
    "        if len(x.shape) == 3:  # [batch_size, 64, 72]\n",
    "            # Reshape to [batch_size, channels, height, width]\n",
    "            # Assuming the input is grayscale (1 channel)\n",
    "            batch_size, height, width = x.shape\n",
    "            x = x.unsqueeze(1)  # Add channel dimension [batch_size, 1, 64, 72]\n",
    "\n",
    "        # Apply patch embedding\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        # Track load-balancing across layers (only if MoE is used)\n",
    "        load_balancing_sum = torch.tensor(0.0, device=x.device)\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.transformer:\n",
    "            x, load_balancing_loss = layer(x, fine_tuning=self.fine_tuning)\n",
    "            load_balancing_sum += load_balancing_loss\n",
    "\n",
    "        load_balancing_loss = (load_balancing_sum / self.num_layer) * self.load_balancing_loss_weight\n",
    "\n",
    "        # Apply output normalization\n",
    "        x = self.output_norm(x)\n",
    "\n",
    "        # Use CLS token for classification\n",
    "        x = x[:, 0]  # Take only the CLS token\n",
    "\n",
    "        # Apply classifier\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x, load_balancing_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def valid_image_folder(path: str) -> bool:\n",
    "    # Check if file starts with '._' or ends with '.DS_Store'\n",
    "    filename = os.path.basename(path)\n",
    "    if filename.startswith(\"._\") or filename == \".DS_Store\": # Stupid MacOS\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Use ImageFolder to automatically label images based on folder names\n",
    "dataset = datasets.ImageFolder(root=dataset_filepath, is_valid_file=valid_image_folder, transform=transform)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint loading\n",
    "def load_checkpoint(model, optimizer, filepath):\n",
    "    checkpoint = torch.load(filepath, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    validation_loss = checkpoint.get('validation_loss', float('inf'))\n",
    "    print(f\"Loaded checkpoint from epoch {epoch} with validation loss {validation_loss:.6f}\")\n",
    "    return epoch\n",
    "\n",
    "# Checkpoint saving\n",
    "def save_checkpoint(model, optimizer, epoch, validation_loss):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch,\n",
    "        'validation_loss': validation_loss\n",
    "    }\n",
    "    torch.save(checkpoint, f\"{save_dir}/vit_checkpoint_epoch_{epoch}.pt\")\n",
    "    # Save best model separately\n",
    "    if epoch == 0 or validation_loss < min(loss_valid):\n",
    "        torch.save(checkpoint, f\"{save_dir}/vit_best_model.pt\")\n",
    "        print(f\"Saved best model with validation loss: {validation_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VisionTransformer(\n",
    "    image_size=(256, 256),    # Your input image dimensions\n",
    "    patch_size=16,           # Size of each patch\n",
    "    in_channels=3,          # We only technically don't have channel value here.\n",
    "    num_classes=1000,       # Number of output classes\n",
    "    num_layer=3,            # Number of transformer layers\n",
    "    hidden_dim=512,        # Hidden dimension\n",
    "    expansion_factor=8,     # Expansion factor for FFN\n",
    "    head_dim=64,            # Dimension of each attention head\n",
    "    q_head=8,              # Number of query heads\n",
    "    kv_head=2,              # Number of key/value heads\n",
    "    use_moe=False           # Whether to use Mixture of Experts\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(vit.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(train_loader), eta_min=1e-5)\n",
    "\n",
    "# Load checkpoint if available\n",
    "current_epoch = 0\n",
    "if checkpoint_filepath is not None and checkpoint_filepath != \"\":\n",
    "    current_epoch = load_checkpoint(vit, optimizer, checkpoint_filepath) + 1\n",
    "\n",
    "print(f\"This model has {sum(p.numel() for p in vit.parameters())} parameters.\")\n",
    "print(f\"Training on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize loss tracking lists\n",
    "loss_train = []\n",
    "loss_valid = []\n",
    "accuracy_train = []\n",
    "accuracy_valid = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(current_epoch, epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    \n",
    "    # Training phase\n",
    "    vit.train()\n",
    "    loss_train_epoch = []\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    # Sliding windows to store metrics for the last 1000 iterations\n",
    "    recent_losses = deque(maxlen=1000)\n",
    "    recent_corrects = deque(maxlen=1000)\n",
    "    recent_totals = deque(maxlen=1000)\n",
    "    \n",
    "    for i, (inputs, targets) in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        outputs, load_balancing_loss = vit(inputs)\n",
    "        loss = criterion(outputs, targets) + load_balancing_loss\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(vit.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Record loss for epoch statistics\n",
    "        loss_train_epoch.append(loss.item())\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        batch_correct = (predictions == targets).sum().item()\n",
    "        batch_total = targets.size(0)\n",
    "        \n",
    "        # Update running totals for epoch statistics\n",
    "        total_train += batch_total\n",
    "        correct_train += batch_correct\n",
    "        \n",
    "        # Add current batch metrics to the sliding windows\n",
    "        recent_losses.append(loss.item())\n",
    "        recent_corrects.append(batch_correct)\n",
    "        recent_totals.append(batch_total)\n",
    "        \n",
    "        # Calculate global iteration number\n",
    "        iteration = epoch * len(train_loader) + i\n",
    "        \n",
    "        # Report metrics every 1000 iterations\n",
    "        if (iteration + 1) % 1000 == 0:\n",
    "            # Calculate average metrics over the last 1000 iterations\n",
    "            avg_loss = sum(recent_losses) / len(recent_losses)\n",
    "            avg_accuracy = 100 * sum(recent_corrects) / sum(recent_totals)\n",
    "            \n",
    "            print(f\"Iteration {iteration + 1} - Avg Loss: {avg_loss:.6f}, Avg Accuracy: {avg_accuracy:.2f}%\")\n",
    "    \n",
    "    # Calculate epoch statistics\n",
    "    epoch_loss = np.mean(loss_train_epoch)\n",
    "    epoch_accuracy = 100 * correct_train / total_train\n",
    "    loss_train.append(epoch_loss)\n",
    "    accuracy_train.append(epoch_accuracy)\n",
    "    \n",
    "    # Validation phase\n",
    "    vit.eval()\n",
    "    loss_val_epoch = []\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(val_loader, desc=\"Validation\"):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs, load_balancing_loss = vit(inputs)\n",
    "            loss = criterion(outputs, targets) + load_balancing_loss\n",
    "            \n",
    "            # Record loss\n",
    "            loss_val_epoch.append(loss.item())\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "            total_val += targets.size(0)\n",
    "            correct_val += (predictions == targets).sum().item()\n",
    "    \n",
    "    # Calculate epoch validation statistics\n",
    "    epoch_val_loss = np.mean(loss_val_epoch)\n",
    "    epoch_val_accuracy = 100 * correct_val / total_val\n",
    "    loss_valid.append(epoch_val_loss)\n",
    "    accuracy_valid.append(epoch_val_accuracy)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f\"Training - Loss: {epoch_loss:.6f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "    print(f\"Validation - Loss: {epoch_val_loss:.6f}, Accuracy: {epoch_val_accuracy:.2f}%\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(vit, optimizer, epoch, epoch_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final evaluation on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "vit.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in tqdm(test_loader, desc=\"Testing\"):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        outputs, load_balancing_loss = vit(inputs)\n",
    "        loss = criterion(outputs, targets) + load_balancing_loss\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        predictions = torch.argmax(outputs, dim=1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predictions == targets).sum().item()\n",
    "\n",
    "avg_test_loss = test_loss / len(test_loader)\n",
    "test_accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"Test set - Loss: {avg_test_loss:.6f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Save final model\n",
    "torch.save({\n",
    "    'model_state_dict': vit.state_dict(),\n",
    "    'test_accuracy': test_accuracy,\n",
    "    'test_loss': avg_test_loss\n",
    "}, f\"{save_dir}/vit_final_model.pt\")\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
